# base on https://github.com/hsuanchia/PTT-Gossiping-Chatbot/blob/main/crawler.py
# crawling data from PTT

#!/usr/bin/python
# -*- coding: utf-8 -*-
# https://www.ptt.cc/bbs/AllTogether/index3970.html -> 2021.11.01
# https://www.ptt.cc/bbs/AllTogether/index4038.html-> 2022.12.31
# https://www.ptt.cc/bbs/lesbian/index3324.html -> 2021.01.01
# https://www.ptt.cc/bbs/lesbian/index3564.html -> 2022.12.31

import requests,  json
from bs4 import BeautifulSoup
from tqdm import tqdm

# enter the url of the board
ptt = "https://www.ptt.cc/bbs/AllTogether/index"
pttcc = "https://www.ptt.cc"

def get_article(start,end,name='tmp'):
    index = 1
    article_output, pushes_output = [], []
    for i in tqdm(range(start,end)):
        url = ptt + (str)(i) + '.html'
        r = requests.get(url)    
        r.encoding = 'utf-8'
        soup = BeautifulSoup(r.text, 'html.parser')
        title = soup.find_all('div',class_='title')
        # print(title)
        for t in tqdm(title):
            article_info = {}
            if(t.find('a') == None):
                continue
            website = t.find('a')['href']
            site = pttcc + website
            res = requests.get(site)
            res.encoding = 'utf-8'
            art_soup = BeautifulSoup(res.text, 'html.parser')

            ### Get article info
            meta_data = art_soup.select('span.article-meta-value')
            if(len(meta_data) != 4):
                continue
            article_info['author'] = meta_data[0].text
            article_info['board'] = meta_data[1].text
            article_info['title'] = meta_data[2].text
            article_info['time'] = meta_data[3].text

            ### Set condition of certain keywords in title
            if(('徵女' not in article_info['title'])):
                continue

            ### Get article content
            contain = art_soup.find(id = 'main-container')
            article_text = contain.text.split("※ 發")[0]
            tmp = article_text.split("\n")[2:]
            content = ""
            for i in tmp:
                if(len(i) < 4):
                    content += i + '\n'
                else:
                    if(i[:4] != 'http'):
                        content += i + '\n'
            article_info['content'] = content
            article_output.append(article_info)
            
    output = {}
    for i in range(len(article_output)):
        output[str(i)] = {}
        output[str(i)]['article'] = article_output[i]
        
    with open(name+'.json','wb+') as pf:
        pf.write(json.dumps(output,indent=4,ensure_ascii=False).encode('utf-8'))

### Index may change day by day
get_article(3970,4038,name='tmp')

